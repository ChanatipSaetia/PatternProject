{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import Imputer, StandardScaler, MinMaxScaler, OneHotEncoder\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from preprocessing import AutoEncoder, ItemSelector, ReplaceMissing\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from evaluation import gini, gini_normalized\n",
    "import pickle\n",
    "\n",
    "from keras.layers import Input, Dense, Embedding, concatenate, Flatten, Activation, Dropout\n",
    "from keras.models import Model\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow import set_random_seed\n",
    "set_random_seed(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/train.csv')\n",
    "df = df.drop('id', axis=1)\n",
    "\n",
    "df_test = pd.read_csv('data/test.csv')\n",
    "df_test = df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "target = df['target'].as_matrix()\n",
    "df = df.drop('target', axis=1)\n",
    "df_test_temp = df_test\n",
    "df_test = df_test.drop('id', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cols_use = [c for c in df.columns if (not c.startswith('ps_calc_'))]\n",
    "df = df[cols_use]\n",
    "df_test = df_test[cols_use]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# delete almost 1 value in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filter_more_frequent = []\n",
    "for i in df:\n",
    "    if(np.sum(df[i] == df[i].mode()[0]) / len(df) <= 0.95):\n",
    "        filter_more_frequent.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = df[filter_more_frequent]\n",
    "df_test = df_test[filter_more_frequent]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter many missing out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filter_index = []\n",
    "for i in df:\n",
    "    if(np.sum(df[i] == -1) / df.shape[0] <= 0.05):\n",
    "        filter_index.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = df[filter_index]\n",
    "df_test = df_test[filter_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# create each map of feature "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "maps_of_feature = { 'binary': [], 'nominal': [], 'interval': [], 'ordinal': []}\n",
    "for f, i in zip(df.columns, range(len(df.columns))):\n",
    "    # Defining the level\n",
    "    if 'bin' in f:\n",
    "        maps_of_feature['binary'].append(i-1)\n",
    "    elif f == 'target':\n",
    "        print(f, i)\n",
    "    elif 'cat' in f or f == 'id':\n",
    "        if(f == 'ps_car_08_cat'):\n",
    "            maps_of_feature['binary'].append(i-1)\n",
    "        else:\n",
    "            maps_of_feature['nominal'].append(i-1)\n",
    "    elif df[f].dtype == float:\n",
    "        maps_of_feature['interval'].append(i-1)\n",
    "    elif df[f].dtype == int:\n",
    "        maps_of_feature['ordinal'].append(i-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_validate = df.as_matrix()\n",
    "test = df_test.as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def balance_train(x_train,y_train):\n",
    "    x_true = x_train[y_train==1]\n",
    "    y_true = y_train[y_train==1]\n",
    "    x_false = x_train[y_train==0]\n",
    "    y_false = y_train[y_train==0]\n",
    "    idx_false = np.random.choice(len(y_false),len(y_true))\n",
    "    x_false = x_false[idx_false]\n",
    "    y_false = y_false[idx_false]\n",
    "    x_train = np.concatenate((x_true,x_train),axis=0)\n",
    "    y_train = np.concatenate((y_true,y_train),axis=0)\n",
    "    \n",
    "    idx = np.arange(len(x_train))\n",
    "    np.random.shuffle(idx)\n",
    "    x_train = x_train[idx]\n",
    "    y_train = y_train[idx]\n",
    "    \n",
    "    print(np.sum(y_train == 0))\n",
    "    print(\"Train count: \"+str(len(y_train)))\n",
    "    unique, counts = np.unique(y_train, return_counts=True)\n",
    "#     print(dict(zip(unique, counts)))\n",
    "    print(\"0 (%): \"+str(counts[0]/y_train.shape[0]*100))\n",
    "    print(\"1 (%): \"+str(counts[1]/y_train.shape[0]*100))\n",
    "    return x_train,y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# create validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def createPipeline():\n",
    "    \n",
    "    PipelineForNumberic = Pipeline([\n",
    "                                ('select_numeric', ItemSelector(maps_of_feature['interval']))\n",
    "                                ,('replace_mean', Imputer(missing_values=-1, strategy='mean'))\n",
    "                                ,('scaler', MinMaxScaler())\n",
    "                               ])\n",
    "\n",
    "    PipelineForOrdinal = Pipeline([\n",
    "                                    ('select_ordinal', ItemSelector(maps_of_feature['ordinal'] + maps_of_feature['binary']))\n",
    "                                    ,('scaler', MinMaxScaler())\n",
    "                                ])\n",
    "\n",
    "    PipelineForCategorical = Pipeline([\n",
    "                                    ('select_categorical', ItemSelector(maps_of_feature['nominal'] ))\n",
    "    #                                 ,('replace_mode', ReplaceMissing(value=999))\n",
    "    #                                 ,('onehot', OneHotEncoder(sparse=False, handle_unknown='ignore'))\n",
    "    #                                 ,('autoencoder300', AutoEncoder(80, 300).cuda())\n",
    "    #                                 ,('autoencoder100', AutoEncoder(300, 100).cuda())\n",
    "                                   ])\n",
    "    \n",
    "    rest_data_preprocessing = Pipeline([\n",
    "            ('data_pre', FeatureUnion(\n",
    "                [\n",
    "                    ('numeric_preprocessing', PipelineForNumberic), \n",
    "                    ('ordinal_preprocessing', PipelineForOrdinal),\n",
    "                ])\n",
    "            ),\n",
    "            ('variance_out', VarianceThreshold(threshold=0.01))\n",
    "    ])\n",
    "    \n",
    "    return rest_data_preprocessing, PipelineForCategorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_features = [];\n",
    "cat_features_ncat = {};\n",
    "\n",
    "for col in df.columns:\n",
    "    if 'cat' in col:\n",
    "        if(col != 'ps_car_08_cat'):\n",
    "            cat_features.append(col)\n",
    "        \n",
    "for col in cat_features:\n",
    "    values = df[col].values\n",
    "    cat_features_ncat[col] = len(set(values))\n",
    "#     print(cat_features_ncat[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# em_dims = {}\n",
    "# for cat in cat_features:\n",
    "#     n_dim = cat_features_ncat[cat]\n",
    "#     em_dim = np.log2(n_dim) + 1\n",
    "#     em_dim = int(em_dim)\n",
    "#     em_dims[cat] = em_dim\n",
    "#     print(n_dim, em_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "em_dims = {'ps_ind_02_cat': 3,\n",
    "          'ps_ind_04_cat': 2,\n",
    "          'ps_ind_05_cat': 5,\n",
    "          'ps_car_01_cat': 7,\n",
    "          'ps_car_02_cat': 2,\n",
    "          'ps_car_03_cat': 2,\n",
    "          'ps_car_04_cat': 5,\n",
    "          'ps_car_05_cat': 2,\n",
    "          'ps_car_06_cat': 8,\n",
    "          'ps_car_07_cat': 2,\n",
    "          'ps_car_09_cat': 3,\n",
    "          'ps_car_10_cat': 2,\n",
    "          'ps_car_11_cat': 10}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_embedding_network():\n",
    "    em_inputs = []\n",
    "    em_outs = []\n",
    "    for cat in cat_features:\n",
    "        n_dim = cat_features_ncat[cat]\n",
    "        em_dim = em_dims[cat]\n",
    "        em_input = Input(shape=(1,), name = cat+'_input')\n",
    "        em_inputs.append(em_input)\n",
    "        em_out = Embedding(n_dim, em_dim, input_length = 1, name = cat+'_embedding')(em_input)\n",
    "        em_out = Flatten()(em_out)\n",
    "        em_outs.append(em_out)\n",
    "\n",
    "    # rest of input in model\n",
    "    n_rest_input = rest_preprocessed_data.shape[1]\n",
    "    rest_input = Input(shape = (n_rest_input,), name = 'rest_input') \n",
    "    rest_out = Dense(16)(rest_input)\n",
    "    print(n_rest_input)\n",
    "\n",
    "    # concatenate input and embedding things\n",
    "    concat_layer_out = concatenate([rest_out] + em_outs, name = 'concatenated_layer')\n",
    "    all_em_out = concatenate(em_outs, name='all_em_output')\n",
    "\n",
    "    # some NN thing\n",
    "    embed = Dense(80, name = 'encode')(concat_layer_out)\n",
    "    embed = Activation('relu')(embed)\n",
    "    embed = Dropout(0.35)(embed)\n",
    "\n",
    "    x = Dense(20)(embed)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Dropout(0.15)(x)\n",
    "\n",
    "    x = Dense(10)(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Dropout(0.15)(x)\n",
    "\n",
    "    output = Dense(1, activation = 'sigmoid', name = 'output')(x)\n",
    "\n",
    "    model = Model(inputs = [rest_input] + em_inputs,\n",
    "                 outputs = [output])\n",
    "\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare input "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ChangeFormat():\n",
    "    \n",
    "    def train_change(self, cate, rest):\n",
    "        all_input = {}\n",
    "        self.val_maps = []\n",
    "        for i, n in enumerate(maps_of_feature['nominal']):\n",
    "            cat = df.columns[n+1]\n",
    "            raw_vals = np.unique(cate[: , i])\n",
    "            val_map = {}\n",
    "            for j in range(len(raw_vals)):\n",
    "                val_map[raw_vals[j]] = j\n",
    "            self.val_maps.append(val_map)\n",
    "            all_input[cat+'_input'] = cate[:, i]\n",
    "        all_input['rest_input'] = rest\n",
    "        return all_input\n",
    "    \n",
    "    def change(self, cate, rest):\n",
    "        all_input = {}\n",
    "        for i, n in enumerate(maps_of_feature['nominal']):\n",
    "            cat = df.columns[n+1]\n",
    "            val_map = self.val_maps[i]\n",
    "            all_input[cat+'_input'] = cate[:, i]\n",
    "        all_input['rest_input'] = rest\n",
    "        return all_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_pred_test = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "rest_data_preprocessing, PipelineForCategorical = createPipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "501828\n",
      "Train count: 539792\n",
      "0 (%): 92.9669205916\n",
      "1 (%): 7.03307940836\n",
      "17\n",
      "Epoch 1/30\n",
      "539792/539792 [==============================] - 2s - loss: 0.3469     \n",
      "Epoch 2/30\n",
      "539792/539792 [==============================] - 2s - loss: 0.2714     \n",
      "Epoch 3/30\n",
      "539792/539792 [==============================] - 2s - loss: 0.2644     \n",
      "Epoch 4/30\n",
      "539792/539792 [==============================] - 2s - loss: 0.2610     \n",
      "Epoch 5/30\n",
      "539792/539792 [==============================] - 2s - loss: 0.2588     \n",
      "Epoch 6/30\n",
      "539792/539792 [==============================] - 2s - loss: 0.2574     - ETA: \n",
      "Epoch 7/30\n",
      "539792/539792 [==============================] - 2s - loss: 0.2561     \n",
      "Epoch 8/30\n",
      "539792/539792 [==============================] - 2s - loss: 0.2553     \n",
      "Epoch 9/30\n",
      "539792/539792 [==============================] - 2s - loss: 0.2552     \n",
      "Epoch 10/30\n",
      "539792/539792 [==============================] - 2s - loss: 0.2543     \n",
      "Epoch 11/30\n",
      "539792/539792 [==============================] - 2s - loss: 0.2538     \n",
      "Epoch 12/30\n",
      "539792/539792 [==============================] - 2s - loss: 0.2534     \n",
      "Epoch 13/30\n",
      "539792/539792 [==============================] - 2s - loss: 0.2531     \n",
      "Epoch 14/30\n",
      "539792/539792 [==============================] - 2s - loss: 0.2528     \n",
      "Epoch 15/30\n",
      "539792/539792 [==============================] - 2s - loss: 0.2526     \n",
      "Epoch 16/30\n",
      "539792/539792 [==============================] - 2s - loss: 0.2524     \n",
      "Epoch 17/30\n",
      "539792/539792 [==============================] - 2s - loss: 0.2521     \n",
      "Epoch 18/30\n",
      "539792/539792 [==============================] - 2s - loss: 0.2519     \n",
      "Epoch 19/30\n",
      "539792/539792 [==============================] - 2s - loss: 0.2516     \n",
      "Epoch 20/30\n",
      "539792/539792 [==============================] - 2s - loss: 0.2515     \n",
      "Epoch 21/30\n",
      "539792/539792 [==============================] - 2s - loss: 0.2513     \n",
      "Epoch 22/30\n",
      "539792/539792 [==============================] - 2s - loss: 0.2510     \n",
      "Epoch 23/30\n",
      "539792/539792 [==============================] - 2s - loss: 0.2506     \n",
      "Epoch 24/30\n",
      "539792/539792 [==============================] - 2s - loss: 0.2505     \n",
      "Epoch 25/30\n",
      "539792/539792 [==============================] - 2s - loss: 0.2502     \n",
      "Epoch 26/30\n",
      "539792/539792 [==============================] - 2s - loss: 0.2503     \n",
      "Epoch 27/30\n",
      "539792/539792 [==============================] - 2s - loss: 0.2501     \n",
      "Epoch 28/30\n",
      "539792/539792 [==============================] - 2s - loss: 0.2498     \n",
      "Epoch 29/30\n",
      "539792/539792 [==============================] - 2s - loss: 0.2498     \n",
      "Epoch 30/30\n",
      "539792/539792 [==============================] - 2s - loss: 0.2496     \n",
      "0.256306374422\n"
     ]
    }
   ],
   "source": [
    "kfold = StratifiedKFold(n_splits=8,\n",
    "                        random_state=231,\n",
    "                        shuffle=True)\n",
    "for i, (f_ind, outf_ind) in enumerate(kfold.split(train_validate, target)):\n",
    "    train, validate = train_validate[f_ind].copy(), train_validate[outf_ind].copy()\n",
    "    train_target, validate_target = target[f_ind], target[outf_ind]\n",
    "    balanced_train, balanced_train_target = balance_train(train, train_target)\n",
    "\n",
    "    rest_data_preprocessing, PipelineForCategorical = createPipeline()\n",
    "    rest_preprocessed_data = rest_data_preprocessing.fit_transform(balanced_train, balanced_train_target)\n",
    "    cate_preprocessed_data = PipelineForCategorical.fit_transform(balanced_train, balanced_train_target)\n",
    "    rest_preprocessed_data_validate = rest_data_preprocessing.transform(validate)\n",
    "    cate_preprocessed_data_validate = PipelineForCategorical.transform(validate)\n",
    "    \n",
    "    format_changer = ChangeFormat()\n",
    "    all_input_train = format_changer.train_change(cate_preprocessed_data, rest_preprocessed_data)\n",
    "    all_input_validate = format_changer.change(cate_preprocessed_data_validate, rest_preprocessed_data_validate)\n",
    "\n",
    "    val = []\n",
    "    for i in range(1):\n",
    "        model = build_embedding_network()\n",
    "        model.fit(all_input_train, balanced_train_target, epochs=30, batch_size=4096, verbose=1)\n",
    "        intermediate_layer_model = Model(inputs=model.input,\n",
    "                                 outputs=model.get_layer('encode').output)\n",
    "        pred_validate = model.predict(all_input_validate)\n",
    "        val.append(pred_validate.reshape(-1))\n",
    "    print(gini_normalized(validate_target, np.sum(val, axis=0))) \n",
    "    break\n",
    "\n",
    "#     rest_preprocessed_data_test = rest_data_preprocessing.transform(test)\n",
    "#     cate_preprocessed_data_test = PipelineForCategorical.transform(test)\n",
    "#     all_input_test = format_changer.change(cate_preprocessed_data_test, rest_preprocessed_data_test)\n",
    "#     pred_test = model.predict(all_input_test).reshape(-1)\n",
    "#     all_pred_test.append(pred_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_train = intermediate_layer_model.predict(all_input_train)\n",
    "encoded_validate = intermediate_layer_model.predict(all_input_validate)\n",
    "model = LogisticRegression(class_weight='balanced')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight='balanced', dual=False,\n",
       "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
       "          multi_class='ovr', n_jobs=1, penalty='l2', random_state=None,\n",
       "          solver='liblinear', tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(encoded_train, balanced_train_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pred = model.predict(encoded_train)\n",
    "pred_validate = model.predict(encoded_validate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.188575625673\n",
      "0.18005023884\n"
     ]
    }
   ],
   "source": [
    "print(gini_normalized(balanced_train_target, pred))\n",
    "print(gini_normalized(validate_target, pred_validate))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Prepare test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rest_preprocessed_data_test = rest_data_preprocessing.transform(test)\n",
    "# cate_preprocessed_data_test = PipelineForCategorical.transform(test)\n",
    "# all_input_test = changeFormat(cate_preprocessed_data_test, rest_preprocessed_data_test)\n",
    "# pred_test = model.predict(all_input_test).reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result = pd.DataFrame({'id':df_test_temp['id'],'target': pred_test.reshape(-1)})\n",
    "# result.to_csv('data/result_embedding.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessed_data_all = data_preprocessing.transform(train_validate)\n",
    "# preprocessed_data_test = data_preprocessing.transform(test)\n",
    "\n",
    "# with open('data/Preprocessing/train.pickle', 'wb') as f:\n",
    "#     pickle.dump(preprocessed_data_all, f)\n",
    "    \n",
    "# with open('data/Preprocessing/test.pickle', 'wb') as f:\n",
    "#     pickle.dump(preprocessed_data_test, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pred_test = model.predict_proba(preprocessed_data_test)\n",
    "# result = pd.DataFrame({'id':df_test['id'],'target': pred_test[:,1]})\n",
    "# result.to_csv('data/result.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
